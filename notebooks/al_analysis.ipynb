{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    # Get final round results\n",
    "    final_round = curves_df[curves_df[\"round\"] == curves_df[\"round\"].max()]\n",
    "    \n",
    "    # Calculate improvement over random for each strategy-classifier combo (no diversity)\n",
    "    final_no_div = final_round[final_round[\"diversity\"] == False].copy()\n",
    "    \n",
    "    improvement_rows = []\n",
    "    for clf in final_no_div[\"classifier\"].unique():\n",
    "        clf_data = final_no_div[final_no_div[\"classifier\"] == clf]\n",
    "        \n",
    "        random_acc = clf_data[clf_data[\"strategy\"] == \"random\"][\"test_accuracy\"].values\n",
    "        if len(random_acc) > 0:\n",
    "            random_acc = random_acc[0]\n",
    "            \n",
    "            for _, row in clf_data.iterrows():\n",
    "                if row[\"strategy\"] != \"random\":\n",
    "                    improvement = (row[\"test_accuracy\"] - random_acc) / (1 - random_acc) * 100 if random_acc < 1.0 else 0\n",
    "                    improvement_rows.append({\n",
    "                        \"classifier\": clf.upper(),\n",
    "                        \"strategy\": row[\"strategy\"].capitalize(),\n",
    "                        \"random_acc\": f\"{random_acc:.4f}\",\n",
    "                        \"strategy_acc\": f\"{row['test_accuracy']:.4f}\",\n",
    "                        \"absolute_gain\": f\"{row['test_accuracy'] - random_acc:+.4f}\",\n",
    "                        \"relative_improvement_%\": f\"{improvement:+.2f}%\",\n",
    "                    })\n",
    "    \n",
    "    improvement_df = pd.DataFrame(improvement_rows)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"IMPROVEMENT OVER RANDOM BASELINE (Final Round)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize improvement as heatmap\n",
    "    if len(improvement_rows) > 0:\n",
    "        pivot_data = final_no_div.copy()\n",
    "        pivot_data[\"vs_random\"] = pivot_data.apply(\n",
    "            lambda x: (x[\"test_accuracy\"] - final_no_div[(final_no_div[\"classifier\"] == x[\"classifier\"]) & (final_no_div[\"strategy\"] == \"random\")][\"test_accuracy\"].values[0]) if x[\"strategy\"] != \"random\" else 0,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        pivot = pivot_data[pivot_data[\"strategy\"] != \"random\"].pivot_table(\n",
    "            values=\"vs_random\",\n",
    "            index=\"classifier\",\n",
    "            columns=\"strategy\",\n",
    "            aggfunc=\"mean\"\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"RdYlGn\", center=0, \n",
    "                    cbar_kws={\"label\": \"Improvement vs Random\"}, ax=ax)\n",
    "        ax.set_title(\"Improvement Over Random Strategy (Final Round)\")\n",
    "        ax.set_xlabel(\"Sampling Strategy\")\n",
    "        ax.set_ylabel(\"Classifier\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"../artifacts/embedding_cnn/al_experiments/improvement_over_random.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(\"\\nImprovement heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73096943",
   "metadata": {},
   "source": [
    "## Improvement Over Random Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(\"Random vs. Uncertainty-Based Strategies (Averaged Across All Classifiers)\", fontsize=14, fontweight=\"bold\")\n",
    "    \n",
    "    classifiers = curves_df[\"classifier\"].unique()\n",
    "    \n",
    "    for idx, clf in enumerate(classifiers):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        clf_data = curves_df[curves_df[\"classifier\"] == clf]\n",
    "        \n",
    "        strategies_ordered = [\"random\", \"entropy\", \"margin\", \"least_confidence\"]\n",
    "        colors = {\"random\": \"red\", \"entropy\": \"blue\", \"margin\": \"green\", \"least_confidence\": \"orange\"}\n",
    "        linestyles = {\"random\": \"--\", \"entropy\": \"-\", \"margin\": \"-\", \"least_confidence\": \"-\"}\n",
    "        \n",
    "        for strategy in strategies_ordered:\n",
    "            strategy_data = clf_data[clf_data[\"strategy\"] == strategy]\n",
    "            \n",
    "            # Average across diversity settings\n",
    "            avg_data = strategy_data.groupby(\"labeled_size\")[\"test_accuracy\"].mean().reset_index()\n",
    "            \n",
    "            label = f\"{strategy.capitalize()}\"\n",
    "            if strategy == \"random\":\n",
    "                label += \" (Baseline)\"\n",
    "            \n",
    "            ax.plot(avg_data[\"labeled_size\"], avg_data[\"test_accuracy\"], \n",
    "                   marker=\"o\", label=label, linewidth=2.5, color=colors[strategy], \n",
    "                   linestyle=linestyles[strategy], markersize=5)\n",
    "        \n",
    "        if baselines_df is not None:\n",
    "            baseline = baselines_df[baselines_df[\"classifier\"] == clf][\"accuracy\"].values[0]\n",
    "            ax.axhline(y=baseline, color=\"black\", linestyle=\":\", linewidth=2, label=\"Baseline (Full Set)\", zorder=0)\n",
    "        \n",
    "        ax.set_xlabel(\"Labeled Samples\")\n",
    "        ax.set_ylabel(\"Test Accuracy\")\n",
    "        ax.set_title(f\"{clf.upper()}\")\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../artifacts/embedding_cnn/al_experiments/random_vs_uncertainty.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Random vs. Uncertainty comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d442910e",
   "metadata": {},
   "source": [
    "## Random vs. Uncertainty-Based Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35169a59",
   "metadata": {},
   "source": [
    "## AL Strategies Tested\n",
    "- **Random**: Random sampling (baseline for comparison)\n",
    "- **Entropy**: Maximum entropy uncertainty\n",
    "- **Margin**: Smallest margin between top-2 class probabilities\n",
    "- **Least Confidence**: Lowest predicted probability of top class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if summary_df is not None:\n",
    "    # Save summary as CSV\n",
    "    summary_csv = Path(\"../artifacts/embedding_cnn/al_experiments/analysis_summary.csv\")\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    print(f\"Saved summary: {summary_csv}\")\n",
    "    \n",
    "    # Create a summary report text file\n",
    "    report_path = Path(\"../artifacts/embedding_cnn/al_experiments/ANALYSIS_REPORT.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "        f.write(\"ACTIVE LEARNING EXPERIMENTS - ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "        \n",
    "        if metadata:\n",
    "            f.write(\"EXPERIMENT CONFIGURATION:\\n\")\n",
    "            f.write(f\"  Seed Size: {metadata.get('seed_size', 'N/A')}\\n\")\n",
    "            f.write(f\"  Query Size: {metadata.get('query_size', 'N/A')}\\n\")\n",
    "            f.write(f\"  Rounds: {metadata.get('rounds', 'N/A')}\\n\")\n",
    "            f.write(f\"  Timestamp: {metadata.get('timestamp', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"BASELINE RESULTS (Full Training Set):\\n\")\n",
    "        f.write(baselines_df.to_string(index=False) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"FINAL AL RESULTS (Last Round):\\n\")\n",
    "        f.write(summary_df.to_string(index=False) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved report: {report_path}\")\n",
    "    print(\"\\nAll visualizations and results have been exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c69ecd",
   "metadata": {},
   "source": [
    "## 8. Export Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefdd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None and baselines_df is not None:\n",
    "    # Get final round\n",
    "    final_round = curves_df[curves_df[\"round\"] == curves_df[\"round\"].max()]\n",
    "    \n",
    "    # Compute summary stats\n",
    "    summary_rows = []\n",
    "    for _, row in final_round.iterrows():\n",
    "        clf = row[\"classifier\"]\n",
    "        baseline_acc = baselines_df[baselines_df[\"classifier\"] == clf][\"accuracy\"].values[0]\n",
    "        improvement = row[\"test_accuracy\"] - baseline_acc\n",
    "        \n",
    "        summary_rows.append({\n",
    "            \"Classifier\": clf.upper(),\n",
    "            \"Strategy\": row[\"strategy\"].capitalize(),\n",
    "            \"Diversity\": \"Yes\" if row[\"diversity\"] else \"No\",\n",
    "            \"Final Accuracy\": f\"{row['test_accuracy']:.4f}\",\n",
    "            \"Baseline\": f\"{baseline_acc:.4f}\",\n",
    "            \"Improvement\": f\"{improvement:+.4f}\",\n",
    "            \"Improvement %\": f\"{improvement*100:+.2f}%\",\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"FINAL SUMMARY STATISTICS (Last AL Round)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Rankings\n",
    "    summary_numeric = final_round.copy()\n",
    "    summary_numeric[\"baseline_acc\"] = summary_numeric[\"classifier\"].apply(\n",
    "        lambda x: baselines_df[baselines_df[\"classifier\"] == x][\"accuracy\"].values[0]\n",
    "    )\n",
    "    summary_numeric[\"improvement\"] = summary_numeric[\"test_accuracy\"] - summary_numeric[\"baseline_acc\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"TOP 10 BEST CONFIGURATIONS (by Improvement over Baseline)\")\n",
    "    print(\"=\" * 100)\n",
    "    top10 = summary_numeric.nlargest(10, \"improvement\")[[\"classifier\", \"strategy\", \"diversity\", \"test_accuracy\", \"improvement\"]]\n",
    "    for i, row in top10.iterrows():\n",
    "        print(f\"{row['classifier'].upper():15} | {row['strategy']:15} | Diversity: {str(row['diversity']):5} | \"\n",
    "              f\"Acc: {row['test_accuracy']:.4f} | Improvement: {row['improvement']:+.4f}\")\n",
    "    \n",
    "    # Best strategy per classifier\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"BEST STRATEGY PER CLASSIFIER (by Final Accuracy)\")\n",
    "    print(\"=\" * 100)\n",
    "    for clf in classifiers:\n",
    "        clf_data = summary_numeric[summary_numeric[\"classifier\"] == clf]\n",
    "        best = clf_data.nlargest(1, \"test_accuracy\").iloc[0]\n",
    "        print(f\"{clf.upper():15} | Strategy: {best['strategy']:15} | Diversity: {best['diversity']} | Accuracy: {best['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b6ff2",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics and Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    # Get final round accuracy\n",
    "    final_round = curves_df[curves_df[\"round\"] == curves_df[\"round\"].max()]\n",
    "    \n",
    "    # Pivot for heatmap (without diversity dimension for simplicity)\n",
    "    final_no_div = final_round[final_round[\"diversity\"] == False]\n",
    "    pivot = final_no_div.pivot_table(\n",
    "        values=\"test_accuracy\",\n",
    "        index=\"classifier\",\n",
    "        columns=\"strategy\",\n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"RdYlGn\", vmin=0, vmax=1, \n",
    "                cbar_kws={\"label\": \"Final Accuracy\"}, ax=ax)\n",
    "    ax.set_title(\"Final Test Accuracy by Strategy and Classifier (No Diversity)\")\n",
    "    ax.set_xlabel(\"Sampling Strategy\")\n",
    "    ax.set_ylabel(\"Classifier\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../artifacts/embedding_cnn/al_experiments/final_accuracy_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56ec6f",
   "metadata": {},
   "source": [
    "## 6. Heatmap: Final Accuracy by Strategy and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c021869",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    strategies = curves_df[\"strategy\"].unique()\n",
    "    classifiers = curves_df[\"classifier\"].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, strategy in enumerate(strategies):\n",
    "        ax = axes[idx]\n",
    "        strategy_data = curves_df[curves_df[\"strategy\"] == strategy]\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            clf_data = strategy_data[strategy_data[\"classifier\"] == clf]\n",
    "            \n",
    "            # Average across diversity settings\n",
    "            avg_data = clf_data.groupby(\"labeled_size\")[\"test_accuracy\"].mean().reset_index()\n",
    "            ax.plot(avg_data[\"labeled_size\"], avg_data[\"test_accuracy\"], \n",
    "                   marker=\"o\", label=clf.upper(), linewidth=2.5)\n",
    "        \n",
    "        if baselines_df is not None:\n",
    "            for idx_base, clf in enumerate(classifiers):\n",
    "                baseline = baselines_df[baselines_df[\"classifier\"] == clf][\"accuracy\"].values[0]\n",
    "                ax.scatter([baselines_df[baselines_df[\"classifier\"] == clf][\"n_train_samples\"].values[0]], \n",
    "                          [baseline], marker=\"*\", s=500, zorder=5)\n",
    "        \n",
    "        ax.set_xlabel(\"Labeled Samples\")\n",
    "        ax.set_ylabel(\"Test Accuracy\")\n",
    "        ax.set_title(f\"{strategy.capitalize()} Strategy\")\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../artifacts/embedding_cnn/al_experiments/strategies_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Strategies comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438ddb6",
   "metadata": {},
   "source": [
    "## 5. Compare Sampling Strategies (All Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    classifiers = curves_df[\"classifier\"].unique()\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        clf_data = curves_df[curves_df[\"classifier\"] == clf]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "        fig.suptitle(f\"{clf.upper()} - Learning Curves Across All Strategies\", fontsize=14, fontweight=\"bold\")\n",
    "        \n",
    "        strategies = curves_df[\"strategy\"].unique()\n",
    "        for idx, strategy in enumerate(strategies):\n",
    "            ax = axes[idx // 3, idx % 3]\n",
    "            \n",
    "            strategy_data = clf_data[clf_data[\"strategy\"] == strategy]\n",
    "            \n",
    "            for diversity in [False, True]:\n",
    "                div_data = strategy_data[strategy_data[\"diversity\"] == diversity]\n",
    "                if len(div_data) > 0:\n",
    "                    div_label = \"w/ Diversity\" if diversity else \"No Diversity\"\n",
    "                    ax.plot(div_data[\"labeled_size\"], div_data[\"test_accuracy\"], \n",
    "                            marker=\"o\", label=div_label, linewidth=2, markersize=4)\n",
    "            \n",
    "            if baselines_df is not None:\n",
    "                baseline = baselines_df[baselines_df[\"classifier\"] == clf][\"accuracy\"].values[0]\n",
    "                ax.axhline(y=baseline, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Baseline (Full Set)\")\n",
    "            \n",
    "            ax.set_xlabel(\"Labeled Samples\")\n",
    "            ax.set_ylabel(\"Test Accuracy\")\n",
    "            ax.set_title(f\"{strategy.capitalize()} Strategy\")\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide last subplot if odd number of strategies\n",
    "        if len(strategies) < 6:\n",
    "            axes[1, 2].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../artifacts/embedding_cnn/al_experiments/curves_{clf}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"Saved curve plot for {clf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78ceb8",
   "metadata": {},
   "source": [
    "## 4. Learning Curves by Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baselines_df is not None:\n",
    "    # Visualize baselines\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0].bar(baselines_df[\"classifier\"], baselines_df[\"accuracy\"], color=\"steelblue\", alpha=0.8)\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].set_title(\"Baseline Accuracy (Full Training Set)\")\n",
    "    axes[0].set_ylim([0, 1.0])\n",
    "    for i, v in enumerate(baselines_df[\"accuracy\"]):\n",
    "        axes[0].text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontsize=9)\n",
    "    \n",
    "    # F1 comparison\n",
    "    axes[1].bar(baselines_df[\"classifier\"], baselines_df[\"f1_macro\"], color=\"coral\", alpha=0.8)\n",
    "    axes[1].set_ylabel(\"F1 (Macro)\")\n",
    "    axes[1].set_title(\"Baseline F1 Score (Full Training Set)\")\n",
    "    axes[1].set_ylim([0, 1.0])\n",
    "    for i, v in enumerate(baselines_df[\"f1_macro\"]):\n",
    "        axes[1].text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../artifacts/embedding_cnn/al_experiments/baseline_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Baseline Performance Summary:\")\n",
    "    print(baselines_df[[\"classifier\", \"accuracy\", \"f1_macro\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1c045",
   "metadata": {},
   "source": [
    "## 3. Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    # Add human-readable labels\n",
    "    curves_df[\"strategy_label\"] = curves_df[\"strategy\"].str.capitalize()\n",
    "    curves_df[\"diversity_label\"] = curves_df[\"diversity\"].map({True: \"w/ Diversity\", False: \"No Diversity\"})\n",
    "    curves_df[\"config\"] = curves_df[\"strategy_label\"] + \" + \" + curves_df[\"diversity_label\"]\n",
    "    curves_df[\"classifier_label\"] = curves_df[\"classifier\"].map({\n",
    "        \"logreg\": \"Logistic Regression\",\n",
    "        \"random_forest\": \"Random Forest\",\n",
    "        \"svc\": \"SVC\",\n",
    "        \"gbdt\": \"Gradient Boosting\",\n",
    "    })\n",
    "    \n",
    "    # Summary by strategy and classifier\n",
    "    print(\"Unique Strategies:\", curves_df[\"strategy\"].unique())\n",
    "    print(\"Unique Classifiers:\", curves_df[\"classifier\"].unique())\n",
    "    print(\"Diversity configs:\", curves_df[\"diversity\"].unique())\n",
    "    print(f\"\\nTotal combinations: {curves_df['config'].nunique()} x {curves_df['classifier'].nunique()}\")\n",
    "    print(f\"Example configs:\")\n",
    "    for config in curves_df[\"config\"].unique()[:3]:\n",
    "        print(f\"  - {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e21f9b",
   "metadata": {},
   "source": [
    "## 2. Parse and Organize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "if baselines_path.exists():\n",
    "    baselines_df = pd.read_csv(baselines_path)\n",
    "    print(\"Baseline Results (Full Training Set):\")\n",
    "    print(baselines_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Baseline results not found. Run experiments first.\")\n",
    "    baselines_df = None\n",
    "\n",
    "# Load AL curves\n",
    "if curves_path.exists():\n",
    "    curves_df = pd.read_csv(curves_path)\n",
    "    print(f\"\\nAL Curves loaded: {len(curves_df)} rows\")\n",
    "    print(f\"Columns: {list(curves_df.columns)}\")\n",
    "    print(f\"\\nSample:\")\n",
    "    print(curves_df.head())\n",
    "else:\n",
    "    print(\"AL curves not found. Run experiments first.\")\n",
    "    curves_df = None\n",
    "\n",
    "# Load metadata\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path) as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"\\nExperiment Metadata:\")\n",
    "    for k, v in metadata.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    metadata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ae4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "experiments_dir = Path(\"../artifacts/embedding_cnn/al_experiments\")\n",
    "baselines_path = experiments_dir / \"baseline_results.csv\"\n",
    "curves_path = experiments_dir / \"all_al_curves.csv\"\n",
    "metadata_path = experiments_dir / \"experiment_metadata.json\"\n",
    "\n",
    "# Check if paths exist\n",
    "print(f\"Experiments directory exists: {experiments_dir.exists()}\")\n",
    "print(f\"Expected files:\")\n",
    "print(f\"  - Baselines: {baselines_path.exists()}\")\n",
    "print(f\"  - Curves: {curves_path.exists()}\")\n",
    "print(f\"  - Metadata: {metadata_path.exists()}\")\n",
    "\n",
    "if experiments_dir.exists():\n",
    "    print(f\"\\nContents of {experiments_dir}:\")\n",
    "    for f in sorted(experiments_dir.iterdir()):\n",
    "        if f.is_file():\n",
    "            print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61797a39",
   "metadata": {},
   "source": [
    "## 1. Load Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "plt.rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce96b89",
   "metadata": {},
   "source": [
    "# Active Learning Experiment Analysis\n",
    "\n",
    "This notebook visualizes and analyzes comprehensive AL experiments comparing different classifiers and sampling strategies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
