{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c0cecb",
   "metadata": {},
   "source": [
    "# Active Learning Experiment Analysis\n",
    "\n",
    "This notebook visualizes and analyzes AL experiment outputs (learning curves, baselines, strategy comparisons and improvement over random).\n",
    "\n",
    "Order: imports → paths → load → organize → baselines → curves → comparisons → heatmaps → improvement → export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (adjust if you placed outputs elsewhere)\n",
    "experiments_dir = Path('../artifacts/embedding_cnn/al_experiments')\n",
    "baselines_path = experiments_dir / 'baseline_results.csv'\n",
    "curves_path = experiments_dir / 'all_al_curves.csv'\n",
    "metadata_path = experiments_dir / 'experiment_metadata.json'\n",
    "print('Experiments dir:', experiments_dir.resolve())\n",
    "print('Exists:', experiments_dir.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files if present\n",
    "baselines_df = pd.read_csv(baselines_path) if baselines_path.exists() else None\n",
    "curves_df = pd.read_csv(curves_path) if curves_path.exists() else None\n",
    "metadata = json.loads(metadata_path.read_text()) if metadata_path.exists() else None\n",
    "print('baselines:', baselines_path.exists(), 'curves:', curves_path.exists(), 'metadata:', metadata_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50283a68",
   "metadata": {},
   "source": [
    "## Parse and quick sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection\n",
    "if curves_df is not None:\n",
    "    print('AL curves columns:', list(curves_df.columns))\n",
    "    display(curves_df.head())\n",
    "else:\n",
    "    print('No AL curves found; run experiments first.')\n",
    "\n",
    "if baselines_df is not None:\n",
    "    display(baselines_df)\n",
    "else:\n",
    "    print('No baseline results found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174561b",
   "metadata": {},
   "source": [
    "## Baseline metrics (full training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baselines_df is not None:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.barplot(data=baselines_df.sort_values('accuracy', ascending=False), x='classifier', y='accuracy')\n",
    "    plt.title('Baseline accuracy (trained on all labeled data)')\n",
    "    plt.ylim(0,1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Baselines missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef9932",
   "metadata": {},
   "source": [
    "## Learning curves by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    for clf, g in curves_df.groupby('classifier'):\n",
    "        pivot = g.pivot_table(index='round', columns='strategy', values='test_accuracy')\n",
    "        plt.figure(figsize=(8,4))\n",
    "        pivot.plot(title=f'Learning curves — {clf}', xlabel='round', ylabel='test_accuracy')\n",
    "        plt.legend(title='strategy')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print('No curves to plot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bfc3c",
   "metadata": {},
   "source": [
    "## Compare sampling strategies (aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    agg = curves_df.groupby(['strategy','round'])['test_accuracy'].mean().reset_index()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.lineplot(data=agg, x='round', y='test_accuracy', hue='strategy')\n",
    "    plt.title('Mean learning curves across classifiers')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No curves for aggregation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545be37",
   "metadata": {},
   "source": [
    "## Final-round heatmap: accuracy by classifier × strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    last_round = curves_df['round'].max()\n",
    "    final = curves_df[curves_df['round'] == last_round].copy()\n",
    "    pivot_final = final.pivot_table(index='classifier', columns='strategy', values='test_accuracy')\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.heatmap(pivot_final, annot=True, fmt='.3f', cmap='viridis')\n",
    "    plt.title('Final-round accuracy by classifier and strategy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No final results available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cd5ca",
   "metadata": {},
   "source": [
    "## Improvement over random (final round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97136eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curves_df is not None:\n",
    "    last_round = curves_df['round'].max()\n",
    "    final = curves_df[curves_df['round'] == last_round].copy()\n",
    "    rnd = final[final['strategy']=='random'][['classifier','test_accuracy']].rename(columns={'test_accuracy':'random_acc'})\n",
    "    merged_final = final.merge(rnd, on='classifier', how='left')\n",
    "    merged_final['improvement_over_random'] = merged_final['test_accuracy'] - merged_final['random_acc']\n",
    "    pivot_imp = merged_final.pivot_table(index='classifier', columns='strategy', values='improvement_over_random')\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.heatmap(pivot_imp, annot=True, fmt='.3f', center=0, cmap='RdBu_r')\n",
    "    plt.title('Improvement over random (final round)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No curves to compare to random.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1a20d",
   "metadata": {},
   "source": [
    "## Summary and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe05012",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'merged_final' in globals():\n",
    "    summary = merged_final.groupby('strategy').agg(mean_final_acc=('test_accuracy','mean'), mean_imp_over_random=('improvement_over_random','mean')).reset_index()\n",
    "    display(summary.sort_values('mean_final_acc', ascending=False))\n",
    "    out_dir = experiments_dir / 'analysis_outputs'\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summary.to_csv(out_dir / 'strategy_summary.csv', index=False)\n",
    "    pivot_final.to_csv(out_dir / 'final_accuracy_matrix.csv')\n",
    "    pivot_imp.to_csv(out_dir / 'improvement_over_random_matrix.csv')\n",
    "    print('Wrote summary files to', out_dir)\n",
    "else:\n",
    "    print('Nothing to export; run previous cells first.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
